@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{khodak-etal-2018-large,
    title = "A Large Self-Annotated Corpus for Sarcasm",
    author = "Khodak, Mikhail  and
      Saunshi, Nikunj  and
      Vodrahalli, Kiran",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L18-1102",
}

@inproceedings{ghosh-etal-2020-report,
    title = "A Report on the 2020 Sarcasm Detection Shared Task",
    author = "Ghosh, Debanjan  and
      Vajpayee, Avijit  and
      Muresan, Smaranda",
    booktitle = "Proceedings of the Second Workshop on Figurative Language Processing",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.figlang-1.1",
    doi = "10.18653/v1/2020.figlang-1.1",
    pages = "1--11",
}

@inproceedings{lee-etal-2020-augmenting,
    title = "Augmenting Data for Sarcasm Detection with Unlabeled Conversation Context",
    author = "Lee, Hankyol  and
      Yu, Youngjae  and
      Kim, Gunhee",
    booktitle = "Proceedings of the Second Workshop on Figurative Language Processing",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.figlang-1.2",
    doi = "10.18653/v1/2020.figlang-1.2",
    pages = "12--17",
}

@inproceedings{a-d-2020-sarcasm,
    title = "Sarcasm Identification and Detection in Conversion Context using {BERT}",
    author = "A., Kalaivani  and
      D., Thenmozhi",
    booktitle = "Proceedings of the Second Workshop on Figurative Language Processing",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.figlang-1.10",
    doi = "10.18653/v1/2020.figlang-1.10",
    pages = "72--76",
}

@inproceedings{dong-etal-2020-transformer,
    title = "Transformer-based Context-aware Sarcasm Detection in Conversation Threads from Social Media",
    author = "Dong, Xiangjue  and
      Li, Changmao  and
      Choi, Jinho D.",
    booktitle = "Proceedings of the Second Workshop on Figurative Language Processing",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.figlang-1.38",
    doi = "10.18653/v1/2020.figlang-1.38",
    pages = "276--280",
}


@InProceedings{10.1007/978-981-16-1543-6_6,
author="Das, Sourav and Kolya, Anup Kumar",
editor="Pan, Indrajit and Mukherjee, Anirban and Piuri, Vincenzo",
title="Parallel Deep Learning-Driven Sarcasm Detection from Pop Culture Text and English Humor Literature",
booktitle="Proceedings of Research and Applications in Artificial Intelligence",
year="2021",
publisher="Springer Singapore",
address="Singapore",
pages="63--73",
abstract="Sarcasm is a sophisticated waya of wrapping any immanent truth, message, or even mockery within a hilarious manner. The advent of communications using social networks has mass-produced new avenues of socialization. It can be further said that humor, irony, sarcasm, and wit are the four chariots of being socially funny in the modern days. In this paper, we manually extract the sarcastic word distribution features of a benchmark pop culture sarcasm corpus, containing sarcastic dialogues and monologues. We generate input sequences formed of the weighted vectors from such words. We further propose an amalgamation of four parallel deep long-short term networks (pLSTM), each with distinctive activation classifier. These modules are primarily aimed at successfully detecting sarcasm from the text corpus. Our proposed model for detecting sarcasm peaks a training accuracy of 98.95{\%} when trained with the discussed dataset. Consecutively, it obtains the highest of 98.31{\%} overall validation accuracy on two handpicked Project Gutenberg English humor literature among all the test cases. Our approach transcends previous state-of-the-art works on several sarcasm corpora and results in a new gold standard performance for sarcasm detection.",
isbn="978-981-16-1543-6"
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}